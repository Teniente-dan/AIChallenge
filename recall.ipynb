{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting jq\n",
      "  Obtaining dependency information for jq from https://files.pythonhosted.org/packages/dd/be/41f1167ff20ac044e0cf07b6c6da3be23606fc56cae46f8f1f7e0b28cfe5/jq-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading jq-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Downloading jq-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (664 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.7/664.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jq\n",
      "Successfully installed jq-1.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time travel, particularly to the future, may not be completely out of reach due to effects of special relativity. However, travel to the past has complex issues such as the \"grandfather paradox,\" making it highly speculative. I've not conducted experiments myself.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are Stephen Hawking.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Tell me what you think of time travel. \\\n",
    "                    Is it possible or not? Have you conducted any experiments? \\\n",
    "                    Please answer as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(deployment_id='d8e937a03013b4a9', messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: blade guard\n",
      "QUERY: Any problem with blade guard, whats the issue reported and name of the product?\n",
      "\n",
      "RESPONSE:\n",
      "The issue with the blade guard was reported for the Central Quality Industries and Pow-R-Tool saw tables. The problem was that these tables did not previously have a blade guard to help prevent finger and hand injuries. The firm received 6 injury complaints, including 4 reports of finger amputation and 2 reports of finger lacerations associated with the use and operation of the saw tables without the blade guards.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "import json\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='recalls/tools.json',\n",
    "    jq_schema='.messages[].Description'    \n",
    ")\n",
    "\n",
    "documents = loader.load_and_split()\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(proxy_model_name='text-embedding-ada-002')\n",
    "# db = Chroma.from_documents(texts, embedding_model)\n",
    "db = DocArrayInMemorySearch.from_documents(documents, embedding_model)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chat_llm = ChatOpenAI(proxy_model_name='gpt-35-turbo')  \n",
    "\n",
    "qa = RetrievalQA.from_llm( llm=chat_llm, retriever=retriever )\n",
    "\n",
    "# product = \"ToolGuards\"\n",
    "# product = \"Saw Table\"\n",
    "\n",
    "#create a prompt asking for the product to analyze\n",
    "prompt = input(\"Please provide the product name for recall analysis\")\n",
    "#pass the prompt to query string\n",
    "print('QUERY:', prompt)\n",
    "product = prompt\n",
    "# query = f\"Any recall with the {product}, whats the issue reported and name of the product?\"\n",
    "query=f\"Any problem with {product}, whats the issue reported and name of the product?\"\n",
    "print ('QUERY:', query)\n",
    "print('\\nRESPONSE:')\n",
    "print(qa.invoke(query)['result'])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
